(10 samples in batch):
2 layers, mean vectors, 11 epochs, lr 0.01, leaky_relu after sum.
Epoch: 0 Loss: 0.4231858276728389 Time: 3020.3253061771393
Epoch: 1 Loss: 0.40407418586908317 Time: 3008.929650783539
Epoch: 2 Loss: 0.3988796904376376 Time: 2937.488674879074
Epoch: 3 Loss: 0.3976303746329328 Time: 2733.8065478801727
Epoch: 4 Loss: 0.3949473102718597 Time: 2728.371485710144
Epoch: 5 Loss: 0.3943836551682133 Time: 2735.284271001816
Epoch: 6 Loss: 0.3935763786681659 Time: 2731.054616689682
Epoch: 7 Loss: 0.39315173922016466 Time: 2726.479468345642
Epoch: 8 Loss: 0.39038440814958986 Time: 2729.594829082489
Epoch: 9 Loss: 0.3893888656213698 Time: 2727.3563759326935
Epoch: 10 Loss: 0.38920614577923335 Time: 2729.688197851181
Epoch: 11 Loss: 0.3890076157746728 Time: 2742.90630030632

NDCG@20: 0.03483739739335406
Recall@20: 0.07756138608903747



3 layers, mean vectors, 11 epochs, lr 0.01, leaky_relu after sum.
Epoch: 0 Loss: 0.39749176936210173 Loss diff: 0.6025082306378983
Epoch: 1 Loss: 0.38214059258805017 Loss diff: 0.015351176774051567
Epoch: 2 Loss: 0.3809749026913324 Loss diff: 0.001165689896717781
Epoch: 3 Loss: 0.37708544506660135 Loss diff: 0.0038894576247310297
Epoch: 4 Loss: 0.3773844762941134 Loss diff: -0.0002990312275120699
Epoch: 5 Loss: 0.37658354392222365 Loss diff: 0.0008009323718897732
Epoch: 6 Loss: 0.37660402874941584 Loss diff: -2.0484827192190913e-05
Epoch: 7 Loss: 0.376360028248941 Loss diff: 0.00024400050047485022
Epoch: 8 Loss: 0.37705205168319206 Loss diff: -0.0006920234342510656
Epoch: 9 Loss: 0.376423824318534 Loss diff: 0.0006282273646580627
Epoch: 10 Loss: 0.3769117030409805 Loss diff: -0.00048787872244648867
Epoch: 11 Loss: 0.37595078169861074 Loss diff: 0.0009609213423697471

NDCG@20: 0.04424323082288789
Recall@20: 0.10083039228736036














Chronological order:
EXPERIMENTS (100 samples in batch):

3 layers baseline:
NDCG@20: 0.027580366559762624
Recall@20: 0.0578922086890682



New: 4 layers. NEW BASELINE
NDCG@20: 0.029002305101198906
Recall@20: 0.06077594835298622
Epoch: 8 Loss: 0.3923036778997285 Loss diff: 0.0014216128474259793
Epoch: 9 Loss: 0.39171882776822087 Loss diff: 0.0005848501315076082
Epoch: 10 Loss: 0.3917714892823263 Loss diff: -5.266151410543429e-05
Epoch: 11 Loss: 0.39099138440985615 Loss diff: 0.0007801048724701554



New: For each iteration of graphs, what wasn't updated = 0 (users_to_zero, articles_to_zero) after the gr.layer (before or after no difference). Worse. WHY?
NDCG@20: 0.02510698346632024
Recall@20: 0.0567802649845104




New: Adam -> AdamW. NEW BASELINE
NDCG@20: 0.02867951908214612
Recall@20: 0.06321676697294262
Epoch: 7 Loss: 0.39544329599780115 Loss diff: 0.0012777928258344562
Epoch: 8 Loss: 0.39425824107360485 Loss diff: 0.0011850549241962938
Epoch: 9 Loss: 0.3936697722623316 Loss diff: 0.0005884688112732794
Epoch: 10 Loss: 0.3939692523556086 Loss diff: -0.00029948009327701586
Epoch: 11 Loss: 0.3933821250172274 Loss diff: 0.000587127338381177


New: if((previous_loss - loss_sum/iterations) < 0.001):
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001). NEW BASELINE
NDCG@20: 0.030389393675876102
Recall@20: 0.06750070023841163
Epoch: 7 Loss: 0.394591138153642 Loss diff: 0.0012119734022025264
Epoch: 8 Loss: 0.39327051795383616 Loss diff: 0.0013206201998058331
Epoch: 9 Loss: 0.3927531547024754 Loss diff: 0.0005173632513607584
Epoch: 10 Loss: 0.3903153318594059 Loss diff: 0.0024378228430694815
Epoch: 11 Loss: 0.38951421100807426 Loss diff: 0.0008011208513316603


New: leak_ReLU after sum + leak_ReLU after each graph. NEW BASELINE
NDCG@20: 0.03939573071567145
Recall@20: 0.08868006217155401
Epoch: 0 Loss: 0.44686820749446693 Loss diff: 0.553131792505533
Epoch: 1 Loss: 0.4064701938327075 Loss diff: 0.040398013661759435
Epoch: 2 Loss: 0.38503252476343264 Loss diff: 0.021437669069274856
Epoch: 3 Loss: 0.3812487726600562 Loss diff: 0.0037837521033764476
Epoch: 4 Loss: 0.38029101664115234 Loss diff: 0.0009577560189038548
Epoch: 5 Loss: 0.3777902714806817 Loss diff: 0.0025007451604706077
Epoch: 6 Loss: 0.37700361790704195 Loss diff: 0.0007866535736397728
Epoch: 7 Loss: 0.37671714030178577 Loss diff: 0.0002864776052561857
Epoch: 8 Loss: 0.3762221858918446 Loss diff: 0.000494954409941184
Epoch: 9 Loss: 0.37624117654687245 Loss diff: -1.899065502786179e-05
Epoch: 10 Loss: 0.3767872603036859 Loss diff: -0.0005460837568134758
Epoch: 11 Loss: 0.3767486150170434 Loss diff: 3.864528664254685e-05


New: GCNConv -> GATConv - WORSE
NDCG@20: 0.0015424814982171815
Recall@20: 0.00410197528201752
---



New: ReLU after sum + leak_ReLU after each graph - WORSE
NDCG@20: 0.021261875927154096
Recall@20: 0.037066424736512275


New: CLS instead of mean - WORSE
NDCG@20: 0.02912322804944753
Recall@20: 0.05936642853843165


New:  negative_slope=0.1 - WORSE
NDCG@20: 0.02509537982201742
Recall@20: 0.049924456634129415



New: GCNConv -> GATConv - WORSE
NDCG@20: 0.009985095974976922
Recall@20: 0.011175918693564267



New: GCNConv -> GCN2Conv - WORSE
k = 4
NDCG@20: 0.005893671784317618
Recall@20: 0.009676073044498721
or
k = 8
NDCG@20: 0.012376866288760512
Recall@20: 0.018660742644214155



New: GCNConv -> GCN2Conv (GCN2Conv(num_node_features, 0.2, 0.2, layer) + self.conv3(x, x, edge_index)) - WORSE
NDCG@20: 0.014015595564303086
Recall@20: 0.02295450150397059



New: GCNConv -> GCN2Conv (GCN2Conv(num_node_features, 0.2) + self.conv3(x, x, edge_index)) - WORSE
NDCG@20: 0.015626243799887408
Recall@20: 0.02650122849493007
Continue: GCN2Conv(num_node_features, 0.4)
NDCG@20: 0.010184960946209813
Recall@20: 0.02213025987155282



New: GCNConv -> GCN2Conv (GCN2Conv(num_node_features, 0) + self.conv3(x, x, edge_index)) - WORSE
NDCG@20: 0.013765561531510097
Recall@20: 0.0234788851181409